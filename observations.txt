Analysis of Model Performance:
1. Variations in Testing Accuracy over Epochs
* The model starts with an accuracy of 87.61% in the first epoch and rapidly increases to 99.88% by the second epoch. By the fourth epoch, it achieves near-perfect accuracy.
* The accuracy stabilizes around 100% from epoch 7 onward, with occasional minor fluctuations.
2. Variations in Loss over Epochs
* The loss begins at 0.38 in the first epoch and decreases sharply to 0.064 in the second epoch.
* Loss values continue to decline, eventually reaching very small values (e.g., 0.00017 in epoch 16).
* However, there is a slight increase in loss in some later epochs (e.g., epoch 10: 0.0185, epoch 19: 0.0031), potentially signaling overfitting.

Observations on Overfitting
* Signs of Overfitting:
o The model achieves near-perfect accuracy on the training set, with extremely low loss values.
o Despite high testing accuracy, the occasional increase in loss suggests the model may be memorizing patterns in the training data rather than generalizing well.
* How the CNN Handles Overfitting:
o The dropout layer (with a dropout rate of 0.28) acts as a regularization method, randomly deactivating neurons during training to prevent the model from becoming overly reliant on specific features.
o This helps to improve generalization but is likely insufficient in this case due to the simplicity of the dataset and model architecture.

Effect of Adding Another Dropout Layer
Adding another dropout layer (e.g., with a rate of 0.4) after the convolutional layers would likely help reduce overfitting further:
* Why?
o Dropout introduces more randomness during training by preventing over-reliance on specific neurons, particularly in deeper layers.
o By applying dropout earlier in the network (e.g., after convolutional layers), the model learns more robust feature maps.
* Expected Outcome:
o Overfitting would decrease.
o Loss values might stabilize further, and fluctuations in testing loss would likely reduce.

Handling of Shared Structure and Invariance Properties
* Shared Structure Property:
o In CNNs, the convolutional layers share weights across the receptive field (filters), enabling the model to detect patterns such as edges, textures, and shapes across the entire image. This reduces the number of parameters and captures meaningful features effectively.
o In our project, the shared structure property is well-utilized by the two Conv2D layers with filters of size 3x3.
* Invariance Property:
o Invariance refers to the model's ability to recognize patterns regardless of their position, orientation, or scale in the image.
o This is achieved through MaxPooling layers, which downsample feature maps, focusing on the most prominent features and introducing spatial invariance.
The project uses 2x2 MaxPooling layers after each convolutional layer to ensure this property.
